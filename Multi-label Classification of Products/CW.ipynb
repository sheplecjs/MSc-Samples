{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "\n",
    "## DSM030 - April 2021 - CW1\n",
    "\n",
    "*This notebook is designed to load a dataset (product-cat-dataset.csv), then perform basic exploration and data cleaning. A function is defined to process text to prepare for bag-of-words, then the vectorized results are transformed with Term Frequency Inverse Document Frequency. The resulting matrix of features is split into training and testing datasets, and a Naive Bayes classifier is used on the training set to create and save models for each category, sub-category, and sub-sub-category combination. The resulting models are then applied to predict the category labels for the testing set and the model accuracy is scored by level.*\n",
    "\n",
    "### Outline:\n",
    "\n",
    "+ [Load and Explore Data](#load)\n",
    "+ [Data Cleaning](#clean)\n",
    "+ [Text Preparation Function](#process)\n",
    "+ [Feature Extraction with TF-IDF](#feature)\n",
    "+ [Train Test Split and Training](#train)\n",
    "+ [Prediction](#predict)\n",
    "+ [Accuracy by Level](#accuracy)\n",
    "+ [Summary Report](#report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnltk\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstem\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msnowball\u001b[39;00m \u001b[39mimport\u001b[39;00m SnowballStemmer\n\u001b[0;32m----> 8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m CountVectorizer \u001b[39mas\u001b[39;00m CV\n\u001b[1;32m      9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature_extraction\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m \u001b[39mimport\u001b[39;00m TfidfTransformer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from typing import List\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer as CV\n",
    "import random\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "import pickle\n",
    "import os\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "\n",
    "## Load and Explore Data\n",
    "\n",
    "*Data is loaded and we make some observations based on summary statistics, charts, and analysis of aggregation.*\n",
    "\n",
    "##### [Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"product-category-dataset.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interesting... let's check the difference between index 1 and 3...\n",
    "data.iloc[1] == data.iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "+ More labels for the levels than descriptions - must be some missing values in Description\n",
    "+ We can see a duplicate description already and the count vs unique of Description suggests a lot more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's investigate these duplicate descriptions in more detail by counting the number of associated classes\n",
    "grouped = data[data[\"Description\"].duplicated()].groupby(\"Description\")\n",
    "gvalue_counts = grouped.agg({\"Level_1\": pd.Series.nunique, \"Level_2\": pd.Series.nunique, \"Level_3\": pd.Series.nunique})\n",
    "gvalue_counts.describe().style.set_caption(\"Duplicate Descriptions: Number of Class Labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "+ It looks like for some of these duplicates there is just one set of labels for level 1, 2, and 3\n",
    "+ There are a few duplicated descriptions that have different labels indicated by slight standard deviation & max value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at duplicated descriptions with different labels in more detail\n",
    "multiple_labels = gvalue_counts[(gvalue_counts[\"Level_1\"] > 1) | (gvalue_counts[\"Level_2\"] > 1) | (gvalue_counts[\"Level_3\"] > 1)]\n",
    "multiple_labels.style.set_caption(\"Duplicate Descriptions with Ambiguous Labels: Count of Classes by Level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "\n",
    "*In a real-world situation, it would be interesting to confirm that duplicate descriptions are in fact different products and not an artifact of data collection (e.g. looks like a different product to a web scraper, but it's just different color options for the same product). Duplicate descriptions will have a corpus-level impact on IDF weights, so knowing if these are indeed different products might be important not just for ensuring our dataset is realistic.*\n",
    "\n",
    "*Since the labels associated with duplicate descriptions with only one combination of class labels is simply duplicated and not ambiguous, we'll prepare to drop duplicates and retain a single instance of these. For multi-classed duplicates, we'll provide a step in data cleaning to remove these records completely since we cannot validate which labeling combination is ground truth or whether or not thee duplicates represent real-world ambiguity.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the meantime, let's preapre some aggregation for class co-occurence heatmaps\n",
    "count_1 = data[\"Level_1\"].unique()\n",
    "count_2 = data[\"Level_2\"].unique()\n",
    "count_3 = data[\"Level_3\"].unique()\n",
    "\n",
    "cross_1_2 = pd.DataFrame(index=count_1, columns=count_2)\n",
    "cross_2_3 = pd.DataFrame(index=count_2, columns=count_3)\n",
    "\n",
    "for i in count_1:\n",
    "    for c in count_2:\n",
    "        occurance = len(data[(data[\"Level_1\"] == i) & (data[\"Level_2\"] == c)])\n",
    "        cross_1_2.loc[i, c] = occurance\n",
    "\n",
    "for i in count_2:\n",
    "    for c in count_3:\n",
    "        occurance = len(data[(data[\"Level_2\"] == i) & (data[\"Level_3\"] == c)])\n",
    "        cross_2_3.loc[i, c] = occurance\n",
    "\n",
    "cross_1_2 = cross_1_2.astype(float)\n",
    "cross_2_3 = cross_2_3.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, sharex=False, figsize=(12,5))\n",
    "fig.suptitle(\"Class Co-occurrence\")\n",
    "axes[0].set_title(\"Level 1 & Level 2\")\n",
    "axes[1].set_title(\"Level 2 & Level 3\")\n",
    "\n",
    "sns.heatmap(cross_1_2, ax=axes[0], cmap=\"mako\")\n",
    "sns.heatmap(cross_2_3, ax=axes[1], cmap=\"mako\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notes:\n",
    "+ Looks like most classes have one or a very small number of possible sub-classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clean\"></a>\n",
    "\n",
    "## Data Cleaning\n",
    "\n",
    "*Missing values and duplicates are delt with then classes with <10 instances are removed.*\n",
    "\n",
    "##### [Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Double check everything for Na\n",
    "data.isna().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop NaN - we'll save the number of rows for reporting later\n",
    "description_missing_count = len(data[data[\"Description\"].isnull()])\n",
    "data.dropna(inplace=True, how=\"any\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove entirely descriptions with multiple label schemes\n",
    "multiple_label_drop = data[data[\"Description\"].isin(multiple_labels.index)].index\n",
    "multiple_label_drop_count = len(data[data[\"Description\"].isin(multiple_labels.index)].index)\n",
    "data = data.drop(multiple_label_drop)\n",
    "\n",
    "# Drop single-class duplicated descriptions\n",
    "duplicates_start = len(data)\n",
    "data = data.drop_duplicates()\n",
    "duplicates_end = len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop Classes where the number of instances is < 10\n",
    "\n",
    "# Apply to Level_1 \n",
    "class_counts_1 = data[\"Level_1\"].value_counts()\n",
    "classes_to_remove_1 = class_counts_1[class_counts_1 < 10].index\n",
    "data = data[~data[\"Level_1\"].isin(classes_to_remove_1)]\n",
    "\n",
    "# Apply to Level_2\n",
    "class_counts_2 = data[\"Level_2\"].value_counts()\n",
    "classes_to_remove_2 = class_counts_2[class_counts_2 < 10].index\n",
    "data = data[~data[\"Level_2\"].isin(classes_to_remove_2)]\n",
    "\n",
    "# Apply to Level_3\n",
    "class_counts_3 = data[\"Level_3\"].value_counts()\n",
    "classes_to_remove_3 = class_counts_3[class_counts_3 < 10].index\n",
    "data = data[~data[\"Level_3\"].isin(classes_to_remove_3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"process\"></a>\n",
    "\n",
    "## Text Preparation Function\n",
    "\n",
    "*A text preparation function that implements flexible stemming and ngram tokenization.*\n",
    "\n",
    "##### [Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text: str, n: int = 3, stem: bool = True, remove_numbers: bool = False) -> List[str]:\n",
    "    \"\"\"Func to prepare text for Count Vectorization. Converts to lowercase, \n",
    "    removes punctuation marks, removes numeric characters (optional), \n",
    "    performs Snowball steming (optional), tokenizes by word, then generates ngrams.\n",
    "\n",
    "    Args:\n",
    "        text (str): The text to be prepared.\n",
    "        n (int, optional): ngram level. Defaults to 3.\n",
    "        stem (bool, optional): Set to False to disable SnowballStemmer. Defaults to True.\n",
    "        remove_numbers (bool, optional): Set to True to enable number removal. Defaults to False.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of ngram = n tokens. \n",
    "    \"\"\"\n",
    "    clean_tokens = []\n",
    "    w_tokens = []\n",
    "    lower = text.lower()\n",
    "    no_punc = lower.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    tokens = no_punc.split(\" \")\n",
    "    if remove_numbers:\n",
    "        number_re = r\"[0-9]\"\n",
    "        for token in tokens:\n",
    "            token = re.sub(number_re, \"\", token)\n",
    "            if token != \"\":\n",
    "                clean_tokens.append(token)\n",
    "    else:\n",
    "        clean_tokens = tokens\n",
    "    if stem:\n",
    "        stemmer = SnowballStemmer(\"english\")\n",
    "        for token in clean_tokens:\n",
    "            token = stemmer.stem(token)\n",
    "            if token != \"\":\n",
    "                w_tokens.append(token)\n",
    "    else:\n",
    "        for token in clean_tokens:\n",
    "            if token != \"\":\n",
    "                w_tokens.append(token)\n",
    "    # Unless we've asked for ngrams of one, iterate over our word tokens and populate with ngrams of n length\n",
    "    if n > 1:\n",
    "        tokenized_ngrams = []\n",
    "        for i in range(len(w_tokens)):\n",
    "            ngram = w_tokens[i:i+n]\n",
    "            if len(ngram) == n:\n",
    "                tokenized_ngrams.append(\" \".join(ngram))\n",
    "    else:\n",
    "        tokenized_ngrams = w_tokens\n",
    "\n",
    "    return tokenized_ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"feature\"></a>\n",
    "\n",
    "## Feature Extraction with TF-IDF\n",
    "\n",
    "*Descripitions are vectorized using our processing function to preprocess, then pass to IDF transformer. As we spot-check the resulting matricies, we'll remove them from memory to keep things running smoothly and because sparse format is more performant in training steps.*\n",
    "\n",
    "##### [Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Override the default analyzer and rely on process_text for lowercase and tokenization\n",
    "\n",
    "# Settings to assign to process_text\n",
    "ngrams = 2\n",
    "stemm = False\n",
    "remove_numbers = True\n",
    "\n",
    "# Applying settings to a callable process-text func\n",
    "callable_processor = process_text\n",
    "callable_processor.__defaults__ = (ngrams, stemm, remove_numbers)\n",
    "\n",
    "Cvectorizer = CV(analyzer=callable_processor)\n",
    "docs = [text for text in data[\"Description\"]]\n",
    "BoW = Cvectorizer.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that the BoW looks ok\n",
    "text = pd.DataFrame(BoW.toarray(), columns=Cvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This sort will take some time, but if we want re-running will check different parts of the dataframe\n",
    "col = random.randint(0, len(text.columns))\n",
    "text.sort_values(by=text.columns[col],ascending=False).iloc[:10, col:col+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this looks ok, let's remove it from memory\n",
    "if \"text\" in globals():\n",
    "    del text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply term frequency inverse document frequency to the BoW\n",
    "vectorizer = TfidfTransformer(use_idf=True)\n",
    "text_tfidf = vectorizer.fit_transform(BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check that the tf-idf matrix looks ok\n",
    "text_tfidf_pd = pd.DataFrame(text_tfidf.toarray(), columns=Cvectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col = random.randint(100, 10000)\n",
    "text_tfidf_pd.sort_values(by=text_tfidf_pd.columns[col],ascending=False).iloc[:10, col:col+10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this looks ok, let's remove it from memory since our libraries work faster with sparse\n",
    "if \"text_tf_idf_pd\" in globals():\n",
    "    del text_tfidf_pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"train\"></a>\n",
    "\n",
    "## Train Test Split and Training\n",
    "\n",
    "*A train test split is performed, then models are created and saved.*\n",
    "\n",
    "##### [Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test and train sets with specified test set size\n",
    "test_size = 0.28\n",
    "X_train, X_test, y_train, y_test = train_test_split(text_tfidf, data, test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convenience Series for training labels\n",
    "class1 = y_train[\"Level_1\"].astype(str)\n",
    "class2 = y_train[\"Level_2\"].astype(str)\n",
    "class3 = y_train[\"Level_3\"].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model training for the three levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory for storing models if needed\n",
    "try:\n",
    "    os.mkdir(\"Models\")\n",
    "except FileExistsError:\n",
    "    print(\"Models directory exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save model for level 1\n",
    "nb_1 = ComplementNB()\n",
    "level_1 = nb_1.fit(X_train, class1)\n",
    "\n",
    "pickle.dump(level_1, open(\"Models/level_1.pk\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save models for level 2\n",
    "for class_1 in data[\"Level_1\"].unique():\n",
    "    training_labels = class2[class1 == class_1]\n",
    "    level_2 = nb_1.fit(X_train[class1 == class_1], training_labels)\n",
    "    pickle.dump(level_2, open(f\"Models/{class_1}_level_2.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and save models for level 3\n",
    "for class_1 in data[\"Level_1\"].unique():\n",
    "    for class_2 in data[data[\"Level_1\"] == class_1][\"Level_2\"].unique():\n",
    "        training_labels = class3[(class1 == class_1) & (class2 == class_2)]\n",
    "        level_3 = nb_1.fit(X_train[(class1 == class_1) & (class2 == class_2)], training_labels)\n",
    "        pickle.dump(level_3, open(f\"Models/{class_1}_{class_2}_level_3.pk\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "\n",
    "## Prediction\n",
    "\n",
    "*Models are applied to the test set.*\n",
    "\n",
    "##### [Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert from sparse to facilitate iteration\n",
    "X_test = X_test.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this out of the loop since we only need it once\n",
    "with open(\"Models/level_1.pk\", \"rb\") as nb:\n",
    "    model = pickle.load(nb)\n",
    "\n",
    "lvl_1 = []\n",
    "lvl_2 = []\n",
    "lvl_3 = []\n",
    "\n",
    "# Iterate over X_test, apply level 1 model, then check for appropriate model or file for each level\n",
    "for index in range(X_test.shape[0]):\n",
    "    pred_data = X_test[index, :]\n",
    "    \n",
    "    # Level 1\n",
    "    l1 = model.predict(pred_data)\n",
    "    lvl_1.append(l1[0])\n",
    "    \n",
    "    # Level 2\n",
    "    l2_path = \"Models/\" + str(l1[0]) + \"_level_2.pk\"\n",
    "    with open(l2_path, \"rb\") as nb_two:\n",
    "        model_2 = pickle.load(nb_two)\n",
    "    l2 = model_2.predict(pred_data)\n",
    "    lvl_2.append(l2[0])\n",
    "    \n",
    "    # Level 3\n",
    "    l3_path = \"Models/\" + str(l1[0]) + \"_\" + str(l2[0]) + \"_level_3.pk\"\n",
    "    with open(l3_path, \"rb\") as nb_three:\n",
    "        model_3 = pickle.load(nb_three)\n",
    "    l3 = model_3.predict(pred_data)\n",
    "    lvl_3.append(l3[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"accuracy\"></a>\n",
    "\n",
    "## Accuracy by Level\n",
    "##### [Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 1 accuracy\n",
    "accuracy_1 = accuracy_score(y_test[\"Level_1\"], lvl_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 2 accuracy\n",
    "accuracy_2 = accuracy_score(y_test[\"Level_2\"], lvl_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Level 3 accuracy\n",
    "accuracy_3 = accuracy_score(y_test[\"Level_3\"], lvl_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"report\"></a>\n",
    "\n",
    "## Summary Report\n",
    "\n",
    "#### [Back to top](#top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print a summary report\n",
    "print(f\"\"\"\n",
    "Results Summary:\\n\\n\n",
    "Records removed for missing Description - {description_missing_count}\\n\n",
    "Records removed for duplicate descriptions with multiple labels - {multiple_label_drop_count}\\n\n",
    "Records dropped for duplicated descriptions - {duplicates_start - duplicates_end}\\n\n",
    "Level 1 classes removed for < 10 instances - {len(classes_to_remove_1)}\\n\n",
    "Level 2 classes removed for < 10 instances - {len(classes_to_remove_2)}\\n\n",
    "Level 3 classes removed for < 10 instances - {len(classes_to_remove_3)}\\n\n",
    "Bag-of-words created with ngrams={callable_processor.__defaults__[0]}, Snowball stemming? {stemm}. Remove numeric? {remove_numbers}\\n\n",
    "Train and test sets of tf-idf transformed Bag-of-words created with a test size of {test_size}\\n\n",
    "Total number of models created - {len([file for file in os.listdir(\"Models/\") if file.endswith(\".pk\")])}\\n\n",
    "Level 1 accuracy was {accuracy_1}\\n\n",
    "Level 2 accuracy was {accuracy_2}\\n\n",
    "Level 3 accuracy was {accuracy_3}\n",
    "\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "60f784e2cb6f164081675afbd485c52771e47820e9513cb2d428fefa7e52407f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
